import math
import torch
import warnings
import ocl
import torch
import torch.nn as nn
import numpy as np
from scipy.optimize import linear_sum_assignment
import torch.nn.functional as F
from ocl.utils.trees import walk_tree_with_paths
from ocl.visualization_types import Visualization

def log_visualizations(visualzer,logger_experiment,outputs,images,global_step, phase="train"):

    visualizations = {}
    for name, vis in visualzer.items():
        if isinstance(vis,ocl.visualizations.Image):
            visualizations[name] = vis(images)
        elif isinstance(vis,ocl.visualizations.Mask):
            visualizations[name] = vis(mask=outputs.masks_as_image)
        elif isinstance(vis,ocl.visualizations.Segmentation):
            visualizations[name] = vis(image=images,mask=outputs.masks_as_image)
        else:
            NotImplementedError


    visualization_iterator = walk_tree_with_paths(
        visualizations, path=None, instance_check=lambda t: isinstance(t, Visualization)
    )
    for path, vis in visualization_iterator:
        try:
            str_path = ".".join(path)
            vis.add_to_experiment(
                experiment=logger_experiment,
                tag=f"{phase}/{str_path}",
                global_step=global_step,
            )
        except AttributeError:
            # The logger does not support the right data format.
            pass

def get_available_devices():
    sys_gpu = torch.cuda.device_count()

    device = torch.device('cuda:0' if sys_gpu > 0 else 'cpu')
    available_gpus = list(range(sys_gpu))
    return device, available_gpus

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def initialize_weights(*models):
    for model in models:
        for m in model.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1.)
                m.bias.data.fill_(1e-4)
            elif isinstance(m, nn.Linear):
                m.weight.data.normal_(0.0, 0.0001)
                m.bias.data.zero_()


def set_trainable_attr(m, b):
    m.trainable = b
    for p in m.parameters(): p.requires_grad = b


def apply_leaf(m, f):
    c = m if isinstance(m, (list, tuple)) else list(m.children())
    if isinstance(m, nn.Module):
        f(m)
    if len(c) > 0:
        for l in c:
            apply_leaf(l, f)


def set_trainable(l, b):
    apply_leaf(l, lambda m: set_trainable_attr(m, b))

def get_correct_slot(slots,using_softmax=False):
    b,num_slots,num_classes=slots.shape
    if using_softmax:
        slots=torch.softmax(slots,dim=-1)  ## [b,num_slots,num_classes]
    __, pred = torch.max(slots.flatten(1, 2), dim=-1)
    indices=pred// num_classes
    correct_slots_list=[]
    for idx in range(slots.shape[0]):
        correct_slots_list.append(slots[idx,indices[idx]])
    return torch.stack(correct_slots_list,dim=0) ## [b,num_classes]

def multi_correct_slot(slots):
    pred,__=torch.max(slots,dim=1)
    pred=torch.sigmoid(pred).detach()
    return pred

def slot_score(slots,threshold=0.95,exp_type="single"):
    slot_logits=slots.detach().cpu()
    softmax_slot=torch.softmax(slot_logits,dim=-1)
    slot_maximum,indices=torch.max(softmax_slot,dim=-1)
    slot_logits=slot_logits*((slot_maximum>threshold).unsqueeze(-1)) ## filter out task-irrelevant slots
    __, pred = torch.max(slot_logits.flatten(1, 2), dim=-1)
    indices=pred// slots.shape[-1]
    correct_slots_list=[]
    for idx in range(slots.shape[0]):
        correct_slots_list.append(slots[idx,indices[idx]])
    return torch.stack(correct_slots_list,dim=0) ## [b,num_classes]

def slot_max(slots,exp_type,phase="val"):
    if exp_type=="single":
        return get_correct_slot(slots)
    else:
        #batch_size, num_slots, num_classes = slots.shape
        #output = torch.zeros((batch_size, num_classes))
        slot_logits = slots.detach().cpu()
        softmax_slot = torch.softmax(slot_logits, dim=-1)
        slot_maximum, indices = torch.max(softmax_slot, dim=-1)
        # if phase=="ood":
        #     print((slot_maximum>0.95).sum())
        slot_logits = slot_logits * ((slot_maximum >0.95).unsqueeze(-1))
        # print(slot_logits.shape)
        #slot_maximum, indices = torch.max(slot_logits, dim=-1)
        output,__=torch.max(slot_logits.flatten(1,2),dim=-1)

    return output

def multi_correct_slot_2(slots,use_softmax=True,threshold=0.90):
    batch_size,num_slots,num_classes=slots.shape
    output=torch.zeros((batch_size,num_classes))
    slot_logits=slots.detach().cpu()
    if use_softmax:
        softmax_slot=torch.softmax(slot_logits,dim=-1)
    slot_maximum,indices=torch.max(softmax_slot,dim=-1)
    # print(indices.max())
    slot_logits=slot_logits*((slot_maximum>threshold).unsqueeze(-1))
    #print(slot_logits.shape)
    slot_maximum,indices=torch.max(slot_logits,dim=-1)
    #print(indices.max())
    for i in range(batch_size):
        output[i,indices[i]]=slot_maximum[i]

    return output