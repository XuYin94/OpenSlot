models:
  feature_extractor:
    _target_: ocl.feature_extractors.timm.TimmFeatureExtractor
    model_name: vit_base_patch16_224_dino
    pretrained: true
    freeze: true
    feature_level: 12

  conditioning:
    _target_: ocl.conditioning.RandomConditioning
    n_slots: 6
    object_dim: 256


  perceptual_grouping:
    _target_: ocl.perceptual_grouping.SlotAttentionGrouping
    feature_dim: ${.object_dim}
    object_dim: ${models.conditioning.object_dim}
    use_projection_bias: false
    positional_embedding:
      _target_: ocl.neural_networks.wrappers.Sequential
      _args_:
        - _target_: ocl.neural_networks.positional_embedding.DummyPositionEmbed
        - _target_: ocl.neural_networks.build_two_layer_mlp
          input_dim: ${experiment.input_feature_dim}
          output_dim: ${....feature_dim}
          hidden_dim: ${experiment.input_feature_dim}
          initial_layer_norm: true
    ff_mlp:
      _target_: ocl.neural_networks.build_two_layer_mlp
      input_dim: ${..object_dim}
      output_dim: ${..object_dim}
      hidden_dim: 1024
      initial_layer_norm: true
      residual: true

  object_decoder:
    _target_: ocl.decoding.AutoregressivePatchDecoder
    decoder_cond_dim: 768
    use_input_transform: true
    use_decoder_masks: true
    object_dim: 256
    output_dim: ${experiment.input_feature_dim}
    num_patches: 196
    decoder:
      _target_: ocl.neural_networks.build_transformer_decoder
      _partial_: true
      n_layers: 4
      n_heads: 4
      return_attention_weights: true

experiment:
  input_feature_dim: 768